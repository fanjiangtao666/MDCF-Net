# YOLOv5-Lite-IV: A Dual-Stream YOLOv5l Model for Infrared and Visible Light Fusion
# This version implements a novel FusionModule that uses contrastive learning principles
# to separate shared and unique features, followed by attention-based fusion.

# Parameters
nc: 80  # number of classes
depth_multiple: 1.0  # model depth multiple
width_multiple: 1.0  # layer channel multiple
anchors:
  - [10,13, 16,30, 33,23]      # P3/8
  - [30,61, 62,45, 59,119]     # P4/16
  - [116,90, 156,198, 373,326] # P5/32

# Backbone
backbone:
  # --- Infrared Stream (Input: 1-channel) ---
  # Layer 0: Initial convolution
  - [-1, 1, Conv, [64, 6, 2, 2]]      # 0-P1/2
  # Layer 1-2: Downsampling and C3 Block
  - [-1, 1, Conv, [128, 3, 2]]       # 1-P2/4
  - [-1, 3, C3, [128]]               # 2
  # Layer 3-4: Downsampling and C3 Block
  - [-1, 1, Conv, [256, 3, 2]]       # 3-P3/8
  - [-1, 6, C3, [256]]               # 4
  # Layer 5-6: Downsampling and C3 Block
  - [-1, 1, Conv, [512, 3, 2]]       # 5-P4/16
  - [-1, 9, C3, [512]]               # 6
  # Layer 7-9: Downsampling, C3, and SPPF
  - [-1, 1, Conv, [1024, 3, 2]]      # 7-P5/32
  - [-1, 3, C3, [1024]]              # 8
  - [-1, 1, SPPF, [1024, 5]]         # 9

  # --- Visible Stream (Input: 3-channel) ---
  # Note: The 'from' column for this stream uses -2 to explicitly take input from the visible light image.
  # This avoids confusion with the -1 (previous layer) convention.
  # See the model parser in yolo_IV_V2.py for implementation details.
  - [-2, 1, Conv, [64, 6, 2, 2]]      # 10-P1/2
  - [-1, 1, Conv, [128, 3, 2]]       # 11-P2/4
  - [-1, 3, C3, [128]]               # 12
  - [-1, 1, Conv, [256, 3, 2]]       # 13-P3/8
  - [-1, 6, C3, [256]]               # 14
  - [-1, 1, Conv, [512, 3, 2]]       # 15-P4/16
  - [-1, 9, C3, [512]]               # 16
  - [-1, 1, Conv, [1024, 3, 2]]      # 17-P5/32
  - [-1, 3, C3, [1024]]              # 18
  - [-1, 1, SPPF, [1024, 5]]         # 19

  # --- Feature Fusion Neck ---
  # The FusionModule is applied at three different scales.
  # It takes features from both the IR and Visible backbones as input.
  - [[4, 14], 1, FusionModule, [256, 256]]   # 20: Fusion at P3/8 scale
  - [[6, 16], 1, FusionModule, [512, 512]]   # 21: Fusion at P4/16 scale
  - [[9, 19], 1, FusionModule, [1024, 1024]] # 22: Fusion at P5/32 scale

# Head (Detection)
head:
  - [22, 1, Conv, [512, 1, 1]]                  # 23
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]  # 24
  - [[-1, 21], 1, Concat, [1]]                  # 25: cat backbone P4
  - [-1, 3, C3, [512, False]]                   # 26

  - [-1, 1, Conv, [256, 1, 1]]                  # 27
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]  # 28
  - [[-1, 20], 1, Concat, [1]]                  # 29: cat backbone P3
  - [-1, 3, C3, [256, False]]                   # 30 (P3/8-small detection)

  - [-1, 1, Conv, [256, 3, 2]]                  # 31
  - [[-1, 27], 1, Concat, [1]]                  # 32: cat head P4
  - [-1, 3, C3, [512, False]]                   # 33 (P4/16-medium detection)

  - [-1, 1, Conv, [512, 3, 2]]                  # 34
  - [[-1, 23], 1, Concat, [1]]                  # 35: cat head P5
  - [-1, 3, C3, [1024, False]]                  # 36 (P5/32-large detection)

  - [[30, 33, 36], 1, Detect, [nc, anchors]]    # 37: Detect(P3, P4, P5)